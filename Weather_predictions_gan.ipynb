{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "decdb89f-65c2-4f50-8de7-af22f3c71cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d510940-311d-4516-b8af-fcd915f90e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the weather data from CSV\n",
    "data = pd.read_csv(\"weatherHistory.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72979d8-c44e-42c8-bf49-7380b0b4abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['Temperature (C)', 'Apparent Temperature (C)', 'Humidity', 'Wind Speed (km/h)', 'Wind Bearing (degrees)', 'Visibility (km)', 'Pressure (millibars)']]\n",
    "target = data['Precip Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59b3049-8b0f-4f9e-9d3f-567e7d2d30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "target = label_encoder.fit_transform(target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd42521-b13e-4959-9a67-39b80809cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the models\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(label_encoder.classes_)\n",
    "generator = Generator(input_dim, output_dim)\n",
    "discriminator = Discriminator(input_dim + output_dim, 1)  # Modify the input_dim and output_dim\n",
    "\n",
    "# Define the loss functions and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "disc_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "867491a1-5dde-4b1e-b146-7a5aaca623c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator and Discriminator models\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b30aead2-c62a-4656-9297-61536e40cdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Discriminator Loss: 0.6921, Generator Loss: 0.7968\n",
      "Epoch 2/100, Discriminator Loss: 0.6955, Generator Loss: 0.6408\n",
      "Epoch 3/100, Discriminator Loss: 0.6873, Generator Loss: 0.7076\n",
      "Epoch 4/100, Discriminator Loss: 0.7019, Generator Loss: 0.6811\n",
      "Epoch 5/100, Discriminator Loss: 0.6930, Generator Loss: 0.6296\n",
      "Epoch 6/100, Discriminator Loss: 0.6973, Generator Loss: 0.6618\n",
      "Epoch 7/100, Discriminator Loss: 0.7046, Generator Loss: 0.6118\n",
      "Epoch 8/100, Discriminator Loss: 0.7057, Generator Loss: 0.7107\n",
      "Epoch 9/100, Discriminator Loss: 0.6912, Generator Loss: 0.6953\n",
      "Epoch 10/100, Discriminator Loss: 0.6874, Generator Loss: 0.7006\n",
      "Epoch 11/100, Discriminator Loss: 0.6933, Generator Loss: 0.6593\n",
      "Epoch 12/100, Discriminator Loss: 0.6949, Generator Loss: 0.7263\n",
      "Epoch 13/100, Discriminator Loss: 0.6885, Generator Loss: 0.6956\n",
      "Epoch 14/100, Discriminator Loss: 0.6980, Generator Loss: 0.6277\n",
      "Epoch 15/100, Discriminator Loss: 0.6912, Generator Loss: 0.6626\n",
      "Epoch 16/100, Discriminator Loss: 0.7008, Generator Loss: 0.6394\n",
      "Epoch 17/100, Discriminator Loss: 0.7138, Generator Loss: 0.5637\n",
      "Epoch 18/100, Discriminator Loss: 0.7037, Generator Loss: 0.6123\n",
      "Epoch 19/100, Discriminator Loss: 0.6844, Generator Loss: 0.6674\n",
      "Epoch 20/100, Discriminator Loss: 0.6937, Generator Loss: 0.6516\n",
      "Epoch 21/100, Discriminator Loss: 0.6998, Generator Loss: 0.6444\n",
      "Epoch 22/100, Discriminator Loss: 0.6959, Generator Loss: 0.6861\n",
      "Epoch 23/100, Discriminator Loss: 0.7139, Generator Loss: 0.6792\n",
      "Epoch 24/100, Discriminator Loss: 0.7026, Generator Loss: 0.6431\n",
      "Epoch 25/100, Discriminator Loss: 0.6968, Generator Loss: 0.6831\n",
      "Epoch 26/100, Discriminator Loss: 0.6921, Generator Loss: 0.6171\n",
      "Epoch 27/100, Discriminator Loss: 0.6966, Generator Loss: 0.6589\n",
      "Epoch 28/100, Discriminator Loss: 0.6970, Generator Loss: 0.6419\n",
      "Epoch 29/100, Discriminator Loss: 0.6894, Generator Loss: 0.6358\n",
      "Epoch 30/100, Discriminator Loss: 0.6996, Generator Loss: 0.6999\n",
      "Epoch 31/100, Discriminator Loss: 0.6945, Generator Loss: 0.6693\n",
      "Epoch 32/100, Discriminator Loss: 0.6924, Generator Loss: 0.6999\n",
      "Epoch 33/100, Discriminator Loss: 0.6964, Generator Loss: 0.6489\n",
      "Epoch 34/100, Discriminator Loss: 0.6754, Generator Loss: 0.7343\n",
      "Epoch 35/100, Discriminator Loss: 0.6846, Generator Loss: 0.6884\n",
      "Epoch 36/100, Discriminator Loss: 0.7006, Generator Loss: 0.6906\n",
      "Epoch 37/100, Discriminator Loss: 0.6988, Generator Loss: 0.6576\n",
      "Epoch 38/100, Discriminator Loss: 0.6970, Generator Loss: 0.6225\n",
      "Epoch 39/100, Discriminator Loss: 0.6964, Generator Loss: 0.6371\n",
      "Epoch 40/100, Discriminator Loss: 0.7033, Generator Loss: 0.6768\n",
      "Epoch 41/100, Discriminator Loss: 0.6990, Generator Loss: 0.7079\n",
      "Epoch 42/100, Discriminator Loss: 0.7199, Generator Loss: 0.5932\n",
      "Epoch 43/100, Discriminator Loss: 0.6919, Generator Loss: 0.6831\n",
      "Epoch 44/100, Discriminator Loss: 0.6959, Generator Loss: 0.7088\n",
      "Epoch 45/100, Discriminator Loss: 0.7029, Generator Loss: 0.6589\n",
      "Epoch 46/100, Discriminator Loss: 0.6989, Generator Loss: 0.6784\n",
      "Epoch 47/100, Discriminator Loss: 0.6929, Generator Loss: 0.6793\n",
      "Epoch 48/100, Discriminator Loss: 0.6901, Generator Loss: 0.6722\n",
      "Epoch 49/100, Discriminator Loss: 0.6990, Generator Loss: 0.6971\n",
      "Epoch 50/100, Discriminator Loss: 0.6996, Generator Loss: 0.6745\n",
      "Epoch 51/100, Discriminator Loss: 0.7013, Generator Loss: 0.6754\n",
      "Epoch 52/100, Discriminator Loss: 0.6965, Generator Loss: 0.6758\n",
      "Epoch 53/100, Discriminator Loss: 0.6885, Generator Loss: 0.7463\n",
      "Epoch 54/100, Discriminator Loss: 0.6947, Generator Loss: 0.6275\n",
      "Epoch 55/100, Discriminator Loss: 0.6944, Generator Loss: 0.7030\n",
      "Epoch 56/100, Discriminator Loss: 0.6820, Generator Loss: 0.6827\n",
      "Epoch 57/100, Discriminator Loss: 0.6942, Generator Loss: 0.6421\n",
      "Epoch 58/100, Discriminator Loss: 0.6845, Generator Loss: 0.6813\n",
      "Epoch 59/100, Discriminator Loss: 0.7088, Generator Loss: 0.6699\n",
      "Epoch 60/100, Discriminator Loss: 0.6932, Generator Loss: 0.7531\n",
      "Epoch 61/100, Discriminator Loss: 0.6897, Generator Loss: 0.7185\n",
      "Epoch 62/100, Discriminator Loss: 0.6962, Generator Loss: 0.6786\n",
      "Epoch 63/100, Discriminator Loss: 0.6934, Generator Loss: 0.7301\n",
      "Epoch 64/100, Discriminator Loss: 0.7013, Generator Loss: 0.6884\n",
      "Epoch 65/100, Discriminator Loss: 0.6838, Generator Loss: 0.7509\n",
      "Epoch 66/100, Discriminator Loss: 0.7009, Generator Loss: 0.6460\n",
      "Epoch 67/100, Discriminator Loss: 0.6986, Generator Loss: 0.6962\n",
      "Epoch 68/100, Discriminator Loss: 0.7133, Generator Loss: 0.6051\n",
      "Epoch 69/100, Discriminator Loss: 0.6837, Generator Loss: 0.6976\n",
      "Epoch 70/100, Discriminator Loss: 0.6901, Generator Loss: 0.7352\n",
      "Epoch 71/100, Discriminator Loss: 0.6927, Generator Loss: 0.7301\n",
      "Epoch 72/100, Discriminator Loss: 0.7072, Generator Loss: 0.6286\n",
      "Epoch 73/100, Discriminator Loss: 0.6994, Generator Loss: 0.6682\n",
      "Epoch 74/100, Discriminator Loss: 0.6907, Generator Loss: 0.7325\n",
      "Epoch 75/100, Discriminator Loss: 0.6885, Generator Loss: 0.7527\n",
      "Epoch 76/100, Discriminator Loss: 0.6879, Generator Loss: 0.7324\n",
      "Epoch 77/100, Discriminator Loss: 0.6943, Generator Loss: 0.6812\n",
      "Epoch 78/100, Discriminator Loss: 0.6959, Generator Loss: 0.6562\n",
      "Epoch 79/100, Discriminator Loss: 0.6907, Generator Loss: 0.7727\n",
      "Epoch 80/100, Discriminator Loss: 0.6938, Generator Loss: 0.6761\n",
      "Epoch 81/100, Discriminator Loss: 0.6981, Generator Loss: 0.6652\n",
      "Epoch 82/100, Discriminator Loss: 0.6881, Generator Loss: 0.7712\n",
      "Epoch 83/100, Discriminator Loss: 0.6866, Generator Loss: 0.7257\n",
      "Epoch 84/100, Discriminator Loss: 0.7019, Generator Loss: 0.6586\n",
      "Epoch 85/100, Discriminator Loss: 0.6922, Generator Loss: 0.7126\n",
      "Epoch 86/100, Discriminator Loss: 0.6901, Generator Loss: 0.6626\n",
      "Epoch 87/100, Discriminator Loss: 0.6861, Generator Loss: 0.7067\n",
      "Epoch 88/100, Discriminator Loss: 0.6986, Generator Loss: 0.7497\n",
      "Epoch 89/100, Discriminator Loss: 0.6906, Generator Loss: 0.6961\n",
      "Epoch 90/100, Discriminator Loss: 0.6947, Generator Loss: 0.7289\n",
      "Epoch 91/100, Discriminator Loss: 0.6955, Generator Loss: 0.6840\n",
      "Epoch 92/100, Discriminator Loss: 0.6848, Generator Loss: 0.7048\n",
      "Epoch 93/100, Discriminator Loss: 0.6976, Generator Loss: 0.6801\n",
      "Epoch 94/100, Discriminator Loss: 0.7108, Generator Loss: 0.7332\n",
      "Epoch 95/100, Discriminator Loss: 0.6957, Generator Loss: 0.6884\n",
      "Epoch 96/100, Discriminator Loss: 0.6861, Generator Loss: 0.6913\n",
      "Epoch 97/100, Discriminator Loss: 0.7023, Generator Loss: 0.6698\n",
      "Epoch 98/100, Discriminator Loss: 0.7050, Generator Loss: 0.6121\n",
      "Epoch 99/100, Discriminator Loss: 0.6951, Generator Loss: 0.7154\n",
      "Epoch 100/100, Discriminator Loss: 0.6999, Generator Loss: 0.6570\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        # Zero out the gradients\n",
    "        gen_optimizer.zero_grad()\n",
    "        disc_optimizer.zero_grad()\n",
    "\n",
    "        # Get the batch data\n",
    "        real_features = X_train[i:i + batch_size]\n",
    "        real_targets = y_train[i:i + batch_size]\n",
    "\n",
    "        # Generate fake data\n",
    "        fake_targets = generator(real_features)\n",
    "        fake_data = torch.cat((fake_targets, real_features), 1)  # Change the concatenation order\n",
    "\n",
    "        # One-hot encode real targets\n",
    "        real_targets_onehot = torch.zeros((real_targets.size(0), output_dim), dtype=torch.float32)\n",
    "        real_targets_onehot.scatter_(1, real_targets.unsqueeze(1), 1)\n",
    "\n",
    "        real_data = torch.cat((real_targets_onehot, real_features), 1)  # Change the concatenation order\n",
    "\n",
    "        # Train the discriminator\n",
    "        disc_real = discriminator(real_data)\n",
    "        disc_fake = discriminator(fake_data)\n",
    "\n",
    "        disc_loss_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_loss_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        disc_loss = (disc_loss_real + disc_loss_fake) / 2\n",
    "\n",
    "        disc_loss.backward()\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        # Train the generator\n",
    "        fake_targets = generator(real_features)\n",
    "        fake_data = torch.cat((fake_targets, real_features), 1)  # Change the concatenation order\n",
    "        gen_output = discriminator(fake_data)\n",
    "        gen_loss = criterion(gen_output, torch.ones_like(gen_output))\n",
    "\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "    # Print the losses for monitoring\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Discriminator Loss: {disc_loss.item():.4f}, Generator Loss: {gen_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "060c7ff6-7c08-4e81-8897-28858dac2448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5177\n"
     ]
    }
   ],
   "source": [
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    test_targets = generator(X_test)\n",
    "    test_targets = torch.argmax(test_targets, dim=1)\n",
    "    accuracy = (test_targets == y_test).float().mean()\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0223a0e0-e326-4d8e-9a5f-e458a03997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = []\n",
    "with torch.no_grad():\n",
    "    for i in range(100):  # Generate 100 samples\n",
    "        fake_targets = generator(X_test)\n",
    "        fake_data = torch.cat((fake_targets, X_test), 1)\n",
    "        generated_data.append(fake_data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ddcb7611-2dd0-4df2-bf97-7b820d242a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 15:40:35.443013: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-29 15:40:35.444043: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-29 15:40:35.444742: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# Load the RNN model from the H5 file\n",
    "rnn_model = load_model('weather_prediction_model.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b16a7152-cb10-4a16-ae27-559d08bbf96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32/60285 [..............................] - ETA: 1:37   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 15:41:53.754979: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-29 15:41:53.756289: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-29 15:41:53.757162: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60285/60285 [==============================] - 95s 2ms/step\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Select relevant features from the generated data array\n",
    "relevant_generated_data_np = generated_data_np[:, :7]  # Select the first 7 features\n",
    "\n",
    "# Reshape the generated data array to match the expected input shape (batch_size, 1, 7)\n",
    "generated_data_reshaped = relevant_generated_data_np.reshape(-1, 1, 7)\n",
    "\n",
    "# Make predictions using the loaded RNN model\n",
    "with tf.device('/CPU:0'):\n",
    "    rnn_output = rnn_model.predict(generated_data_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aeff4df4-8dd1-4036-af9d-0844a33619d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 3.4728345e-10]\n",
      " [1.0000000e+00 2.8320124e-33]\n",
      " [1.0000000e+00 7.2825305e-36]\n",
      " ...\n",
      " [1.0000000e+00 3.0120832e-20]\n",
      " [1.0000000e+00 3.9179632e-19]\n",
      " [1.0000000e+00 6.1567925e-12]]\n"
     ]
    }
   ],
   "source": [
    "print(rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc2d73-a316-4444-a841-6fef2763c6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
